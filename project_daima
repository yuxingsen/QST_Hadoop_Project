代码
1.数据清洗代码：
package Log_Hadoop_shizhan;

import java.io.IOException;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Locale;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class LogClean {
	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
		Configuration conf = new Configuration();
		Job job = Job.getInstance(conf);
		job.setJarByClass(LogClean.class);
		 
		job.setMapperClass(map.class);
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(Text.class);
		job.setNumReduceTasks(1);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);
	
		FileInputFormat.setInputPaths(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
	     
		job.waitForCompletion(true);
	}
	public static class map extends Mapper<LongWritable, Text, Text, Text>{
         private LogParser logparser=new LogParser();
	     private Text map_key=new Text();
		@Override
		protected void map(LongWritable key, Text value, Context context)
				throws IOException, InterruptedException {
			String log=value.toString();
			String[]user=logparser.parse(log);
			map_key.set(user[0]+"\t"+user[1]+"\t"+user[2]+"\t"+user[3]+"\t"+user[4]);
			context.write(map_key, new Text(""));
		}
		
	}
	public static class LogParser {
		public static final SimpleDateFormat FORMAT = new SimpleDateFormat("d/MMM/yyyy:HH:mm:ss", Locale.ENGLISH);
		public static final SimpleDateFormat dateformat1=new SimpleDateFormat("yyyyMMddHHmmss");

		/**
		 * 解析日志的行记录
		 * @param line
		 * @return 数组含有5个元素，分别是ip、时间、url、手机系统、跳转是否是百度
		 */
		
		public String[] parse(String line){
			String ip = parseIP(line);
			String time;
			try {
				time = parseTime(line);
			} catch (Exception e1) {
				time = "null";
			}
			String url;
			try {
				url = parseURL(line);
			} catch (Exception e) {
				url = "null";
			}
		     String os=parseOS(line);
		     String baidu=parseDangPage(line);
		
			
			return new String[]{ip, time ,url,os,baidu};
		}
		private String parseDangPage(String line){//当前页面
			String url;
			try{
			final int first = line.lastIndexOf("http://www.baidu.com");
			final int last = line.lastIndexOf(")");
			url = line.substring(first, last);
			url="baidu";
			}catch(Exception e){
				url="null";
			}
			return url;
			
		}
		private String parseOS(String line) {
			String os;
			try{
			if(line.contains("Android")){
				os="Android";
			}else if(line.contains("iPhone")){
				os="iPhone";
			}else{
				os="null";
			}
			
			}catch(Exception e){
				os="null";
			}
			return os;
		}
	
		private String parseURL(String line) {
			String pattern="(\\d+\\.\\d+\\.\\d+\\.\\d+).{5}\\[(\\d+\\/[a-zA-Z]+\\/\\d+\\:\\d+\\:\\d+\\:\\d+).{13}\\/((show|musicians).\\d+)";  
			Pattern r = Pattern.compile(pattern);
			 Matcher m = r.matcher(line);
			String url = null;
			 try{
				if (m.find()){
					url=m.group(3);
				}
			 }catch(Exception e){
				 url="null";
			 }
			return url;
		}
		private String parseTime(String line) {
			final int first = line.indexOf("[");
			final int last = line.indexOf("+0800]");
			String time = line.substring(first+1,last).trim();
			try {
				return dateformat1.format(FORMAT.parse(time));
			} catch (ParseException e) {
				e.printStackTrace();
			}
			return "";
		}
		private String parseIP(String line) {
			String ip = line.split("- -")[0].trim();
			return ip;
		}
		
	}
}
批量导入hbase程序：
package Hbase_sqoop;

import java.io.IOException;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.Locale;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.mapreduce.TableOutputFormat;
import org.apache.hadoop.hbase.mapreduce.TableReducer;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Counter; 
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;



public class Hbase {
public static void main(String[] args) throws Exception {
	final Configuration configuration = new Configuration();
	//设置zookeeper
	configuration.set("hbase.zookeeper.property.clientport", "2181");
	configuration.set("hbase.zookeeper.quorum", "vm10-0-0-2.ksc.com");
	//设置hbase表名称
	configuration.set(TableOutputFormat.OUTPUT_TABLE, "Hbase_log");
	//将该值改大，防止hbase超时退出
	configuration.set("dfs.socket.timeout", "180000");
	
	final Job job = new Job(configuration, "HBaseBatchImport");
	
	job.setMapperClass(map.class);
	job.setReducerClass(reduce.class);
	//设置map的输出，不设置reduce的输出类型
	job.setMapOutputKeyClass(LongWritable.class);
	job.setMapOutputValueClass(Text.class);
	
	job.setInputFormatClass(TextInputFormat.class);
	//不再设置输出路径，而是设置输出格式类型
	job.setOutputFormatClass(TableOutputFormat.class);
	
	FileInputFormat.setInputPaths(job, "hdfs://vm10-0-0-2.ksc.com:9000/user/hadoop/hadoop_project/*");
	
	job.waitForCompletion(true);

}
public static class map extends Mapper<LongWritable, Text, LongWritable, Text>{
	 private Text v2 = new Text();
	 private LogParser logparser=new LogParser();
	@Override
	protected void map(LongWritable key, Text value, Context context)
			throws IOException, InterruptedException {
		String log=value.toString();
		String[]user=logparser.parse(log);//String[]{ip, time ,url,os,baidu};
		try {
			String rowKey = user[1]+"/"+user[2];
			String val=user[0]+"\t"+user[1]+"\t"+user[2]+"\t"+user[3]+"\t"+user[4];
			v2.set(rowKey+"\t"+val);
			context.write(key, v2);
		} catch (NumberFormatException e) {
			final Counter counter = context.getCounter("BatchImport", "ErrorFormat");
			counter.increment(1L);
			System.out.println("出错了"+user[0]+" "+e.getMessage());
		}
	}
}
public static class reduce extends TableReducer<LongWritable, Text, NullWritable>{
	public static final SimpleDateFormat dateformat1=new SimpleDateFormat("yyyyMMddHHmmss");
	
	@Override
	protected void reduce(LongWritable key, Iterable<Text> values,
		Context context)
			throws IOException, InterruptedException {
		Date data = null;
		for (Text text : values) {
			final String[] splited = text.toString().split("\t");
			  try {
				data=dateformat1.parse(splited[2]);
			} catch (ParseException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			final Put put = new Put(Bytes.toBytes(splited[0]),(data.getTime())/1000);//row ,ip, time ,url,os,baidu
			put.add(Bytes.toBytes("info"), Bytes.toBytes("ip"), Bytes.toBytes(splited[1]));
			put.add(Bytes.toBytes("info"), Bytes.toBytes("url"), Bytes.toBytes(splited[3]));
			put.add(Bytes.toBytes("info"), Bytes.toBytes("systems"), Bytes.toBytes(splited[4]));
			put.add(Bytes.toBytes("info"), Bytes.toBytes("page"), Bytes.toBytes(splited[5]));
			context.write(NullWritable.get(), put);
		}

	}
	
}
public static class LogParser {
	public static final SimpleDateFormat FORMAT = new SimpleDateFormat("d/MMM/yyyy:HH:mm:ss", Locale.ENGLISH);
	public static final SimpleDateFormat dateformat1=new SimpleDateFormat("yyyyMMddHHmmss");

	/**
	 * 解析日志的行记录
	 * @param line
	 * @return 数组含有5个元素，分别是ip、时间、url、手机系统、跳转是否是百度
	 */
	
	public String[] parse(String line){
		String ip = parseIP(line);
		String time;
		try {
			time = parseTime(line);
		} catch (Exception e1) {
			time = "null";
		}
		String url;
		try {
			url = parseURL(line);
		} catch (Exception e) {
			url = "null";
		}
	     String os=parseOS(line);
	     String baidu=parseDangPage(line);
	
		
		return new String[]{ip, time ,url,os,baidu};
	}
	private String parseDangPage(String line){//当前页面
		String url;
		try{
		final int first = line.lastIndexOf("http://www.baidu.com");
		final int last = line.lastIndexOf(")");
		url = line.substring(first, last);
		url="1";
		}catch(Exception e){
			url="0";
		}
		return url;
		
	}
	private String parseOS(String line) {
		String os;
		try{
		if(line.contains("Android")){
			os="Android";
		}else if(line.contains("iPhone")){
			os="iPhone";
		}else{
			os="other";
		}
		
		}catch(Exception e){
			os="other";
		}
		return os;
	}

	private String parseURL(String line) {
		String pattern="(\\d+\\.\\d+\\.\\d+\\.\\d+).{5}\\[(\\d+\\/[a-zA-Z]+\\/\\d+\\:\\d+\\:\\d+\\:\\d+).{13}\\/((show|musicians|musician)).\\d+";  
		Pattern r = Pattern.compile(pattern);
		 Matcher m = r.matcher(line);
		String url = "other";
		 try{
			if (m.find()){
				url=m.group(3);
			}
		 }catch(Exception e){
			 url="other";
		 }
		return url;
	}
	private String parseTime(String line) {
		final int first = line.indexOf("[");
		final int last = line.indexOf("+0800]");
		String time = line.substring(first+1,last).trim();
		try {
			return dateformat1.format(FORMAT.parse(time));
		} catch (ParseException e) {
			e.printStackTrace();
		}
		return "";
	}
	private String parseIP(String line) {
		String ip = line.split("- -")[0].trim();
		return ip;
	}
	
}
}
创建hive表语句：
  
create external table log_yuxingsen (ip string , times string,url string ,os string ,baidu string ) partitioned by (putdate string ) row format delimited fields terminated by '\t' ;
load data inpath '/user/hadoop/yuxingsen/d2/part-r-00000'  into table log_yuxingsen partition (putdate='2015-10-02');
//统计uv
select count(distinct(ip))from log_yuxingsen where putdate='2015-10-02';
//统计pv
select count(ip)from log_yuxingsen where putdate='2015-10-01';
//统计使用手机系统是ios
select count(ip) from log_yuxingsen where putdate='2015-10-01' and os='iPhone';
select count(ip)from log_yuxingsen where putdate='2015-10-01' and baidu='baidu';
统计次日留存：
SELECT (SELECT COUNT(*) FROM (SELECT DISTINCT ip  FROM  log_yuxingsen  WHERE putdate='2015-10-01')t ,(SELECT DISTINCT ip  FROM  log_yuxingsen  WHERE putdate='2015-10-02') b WHERE t.ip=b.ip)/(SELECT COUNT(*) FROM( SELECT DISTINCT ip FROM  log_yuxingsen WHERE putdate='2015-10-01')h) ;
页面跳转率程序：
 
package Log_Hadoop_shizhan;

import java.io.IOException;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Calendar;
import java.util.Date;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import  org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Counter;
import org.apache.hadoop.mapreduce.Counters;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import Log_Hadoop_shizhan.LogClean.map;
public class TiaoZhuanLv {
	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
		Configuration conf=new Configuration();
		Job job=Job.getInstance(conf);
		job.setJarByClass(TiaoZhuanLv.class);
		
		job.setMapperClass(map.class);
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(Text.class);
		
		job.setCombinerClass(combine.class);
		job.setNumReduceTasks(1);
		job.setReducerClass(reduce.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);
	
		FileInputFormat.setInputPaths(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
	     
		job.waitForCompletion(true);
		
		Counters counters=job.getCounters();
		Counter counte1=counters.findCounter("ddd2","show_count");
		Counter counte2=counters.findCounter("ddd1","music_count");
		Counter counte3=counters.findCounter("ddd3","music_zhuan");
		Counter counte4=counters.findCounter("ddd4","show_zhuan");
		double show=(double)counte4.getValue()/counte1.getValue();
		double music=(double)counte3.getValue()/counte2.getValue();
		System.out.println("show_tiaozhuan"+show);
		System.out.println("music_tiaozhuan"+music);
	}
	public static class map extends Mapper<LongWritable, Text, Text, Text>{
		
		@Override
		protected void map(LongWritable key, Text value, Context context)
				throws IOException, InterruptedException {
			String str=value.toString();
			String []user=str.split("\t");
			context.write(new Text(user[0]), new Text(user[2]+"\t"+user[1]));
		}
	}
	public static class combine extends Reducer<Text, Text, Text, Text>{

		@Override
		protected void reduce(Text key, Iterable<Text> values,Context context)
				throws IOException, InterruptedException {
	        for(Text text:values){
	        	String log=text.toString();
	        	if(log.contains("show")||log.contains("music")){
	        		context.write(key, text);
	        	}
	        }
		}
		
	}
	public static class reduce extends Reducer<Text, Text, Text, Text>{
		 private long count1=0;
		 private long count2=0;
		 private long count3=0;
		 private long count4=0;
         private ArrayList<Date> show_list=new ArrayList<Date>();
         private ArrayList<Date> music_list=new ArrayList<Date>();
         private static final SimpleDateFormat dateformat1=new SimpleDateFormat("yyyyMMddHHmmss");
		@Override
		protected void reduce(Text key, Iterable<Text> values,
			       Context context)
				throws IOException, InterruptedException {
		
	       for(Text value:values){
	    	   String time_lei=value.toString();
	    	   String[]user=time_lei.split("\t");
	    	   try {
		    	   Date date=dateformat1.parse(user[1]);
		    	   if(time_lei.startsWith("show")){
		    		   count1++;
						show_list.add(date);
		    		  
		    	   }else{
		    		   count2++;
		    		   music_list.add(date);
		    	   }
	    	   } catch (ParseException e) {
					e.printStackTrace();
	    		   }
	       }
	       Calendar cal = Calendar.getInstance();   
	        Date dateMax;
	        show:for(Date date_show:show_list){
	        	for(Date date_music:music_list){
	        		  cal.setTime(date_show);   
	      	        cal.add(Calendar.MINUTE, 1);// 24小时制   
	      	        dateMax = cal.getTime();   
	        		if(date_show.getTime()<date_music.getTime()&&date_music.getTime()<dateMax.getTime()){
	        			count3++;
	        			break show;
	        		}
	        	}
	        }
	       
	        music:for(Date date_show:music_list){
	        	for(Date date_music:show_list){
	        		  cal.setTime(date_show);   
	      	        cal.add(Calendar.MINUTE, 1);// 24小时制   
	      	        dateMax = cal.getTime();   
	        		if(date_show.getTime()<date_music.getTime()&&date_music.getTime()<dateMax.getTime()){
	        			count4++;
	        			break music;
	        		}
	        	}
	        	context.write(key, new Text(count3+""));
	        }

		}
		
		@Override
		protected void cleanup(Context context)
				throws IOException, InterruptedException {
			Counter count_1=context.getCounter("ddd1", "music_count");
			count_1.increment(count2);
			Counter count_2=context.getCounter("ddd2", "show_count");
			count_2.increment(count1);
			Counter count_3=context.getCounter("ddd3", "music_zhuan");
			count_3.increment(count4);
			Counter count_4=context.getCounter("ddd4", "show_zhuan");
			count_4.increment(count3);
		}
		
	}
}
实时查询show的程序：
package Hbase_sqoop;

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.KeyValue;
import org.apache.hadoop.hbase.MasterNotRunningException;
import org.apache.hadoop.hbase.ZooKeeperConnectionException;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.filter.CompareFilter;
import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
import org.apache.hadoop.hbase.filter.SubstringComparator;
import org.apache.hadoop.hbase.filter.ValueFilter;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.log4j.BasicConfigurator;
import org.apache.log4j.Level;
import org.apache.log4j.Logger;



public class show_hbase {
	private static Configuration conf=null;
	private static HBaseAdmin admin=null;
	private static Logger log=Logger.getLogger(show_hbase.class);
	private static String hTableName = "Hbase_log";

	
	  @SuppressWarnings("deprecation")	
    public static void main( String[] args ) throws MasterNotRunningException, ZooKeeperConnectionException, IOException
    {
        initLog();
        conf=new Configuration();
        conf.set("hbase.zookeeper.property.clientport", "2181");
    	conf.set("hbase.zookeeper.quorum", "master");//,192.168.2.34,192.168.2.45,192.168.2.44
    	admin = new HBaseAdmin(conf);
    	log.info("Log in");
    	System.out.println(showcount());
    	admin.close();
    }

    private static int showcount() throws IOException{
    	if(tableExist(hTableName)==false){
    		return 0;
    	}
    	int count=0;
    	//通过scan查询数据
    	HTable table=new HTable(conf, Bytes.toBytes(hTableName));
    	 Scan scan=new Scan();
    	 scan.addColumn("info".getBytes(), "url".getBytes());
    	 ValueFilter fileter=new ValueFilter(CompareFilter.CompareOp.EQUAL,                     
    		        new SubstringComparator("show"));
    	 scan.setFilter(fileter);
    	 ResultScanner results = table.getScanner(scan);
    	 Result result=null;
    	while( (result=results.next())!=null){
    		count++;
    	}
       
         return count;
    }
	
	
    private static boolean tableExist(String tableName) throws IOException{
		//判断表是否存在
		return admin.tableExists(tableName);	
	}
    
    private static void initLog(){
    	BasicConfigurator.configure();
    	log.setLevel(Level.INFO);
    }
}

